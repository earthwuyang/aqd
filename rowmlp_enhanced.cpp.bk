/************************************************************************************
 * rowmlp_enhanced.cpp  –  same training pipeline, richer feature set
 *   • 24 features (access-type histogram, selectivity stats, cost ratios,
 *     using-index coverage, plan-shape flags, join-depth, fan-out, …)
 *   • FANN MLP: 24 -> hidden -> 1
 *   • Everything else (showProgressBar, runtime metrics, CSV/ROC, etc.)
 *     copied verbatim from your baseline.
 *
 *   g++ -std=c++11 -I/path/to/json/single_include -lfann -o rowmlp_enhanced rowmlp_enhanced.cpp
 ************************************************************************************/

#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
#include <map>
#include <random>
#include <algorithm>
#include <numeric>
#include <cctype>
#include <chrono>
#include <cmath>
#include <cstring>
#include <cstdlib>

#include <dirent.h>
#include <sys/stat.h>

#include <fann.h>
#include "json.hpp"
using json = nlohmann::json;

/* -------------------------------------------------------------------------- */
static void logInfo (const std::string& s){ std::cout  <<"[INFO]  "<<s<<'\n'; }
static void logWarn (const std::string& s){ std::cerr  <<"[WARN]  "<<s<<'\n'; }
static void logError(const std::string& s){ std::cerr  <<"[ERROR] "<<s<<'\n'; }

/* -------------------------------------------------------------------------- */
bool fileExists(const std::string& p){ std::ifstream f(p); return f.good(); }

void showProgressBar(size_t cur,size_t tot,size_t w=50){
    double f = (tot==0)?1.0:double(cur)/tot;
    int filled = int(f*w);
    std::cout<<"\r[";
    for(int i=0;i<filled;i++) std::cout<<'=';
    for(int i=filled;i<(int)w;i++) std::cout<<' ';
    std::cout<<"] "<<int(f*100)<<"% ("<<cur<<'/'<<tot<<')';
    if(cur>=tot) std::cout<<'\n';
    std::fflush(stdout);
}

/* -------------------------------------------------------------------------- */
double parseNum(const json& j,const std::string& k){
    if(!j.contains(k)) return 0.0;
    try{
        if(j[k].is_string())                return std::stod(j[k].get<std::string>());
        if(j[k].is_number())                return j[k].get<double>();
    }catch(...){ logWarn("bad num "+k); }
    return 0.0;
}

double bytesFrom(const std::string& s){
    if(s.empty()) return 0.0;
    std::string t=s; while(!t.empty() && isspace(t.back())) t.pop_back();
    double f=1.0; char suf=t.back();
    if(suf=='G'){f=1e9;t.pop_back();}
    else if(suf=='M'){f=1e6;t.pop_back();}
    else if(suf=='K'){f=1e3;t.pop_back();}
    try{ return std::stod(t)*f; }catch(...){ return 0.0; }
}

/* -------------------------------------------------------------------------- */
constexpr int NUM_FEATS = 24;          /* <-- NEW feature width                */
double  safeLog1p(double v){ return std::log1p(std::max(0.0,v)); }

/* -------------------------------------------------------------------------- */
struct FeatureAgg {
    /* per-table aggregates */
    double reSum=0,rpSum=0,fSum=0,rcSum=0,ecSum=0,pcSum=0,drSum=0;
    double selMin=1e30, selMax=0.0, selSum=0.0;
    double ratioSum=0.0, ratioMax=0.0;
    int    cnt=0;

    /* access-type histogram & flags */
    int cntRange=0,cntRef=0,cntEq=0,cntIdx=0,cntFull=0;
    int usingIdx=0;

    /* plan shape */
    int maxDepth=0;
    double fanoutMax=0.0;
    bool hasGroup=false, hasOrder=false, hasTemp=false;
};

void recursePlan(const json& node, FeatureAgg& A,
                 int depth, double parentRP)
{
    if(node.is_object()){
        if(node.contains("table")){
            const auto& tbl = node["table"];
            double re = parseNum(tbl,"rows_examined_per_scan");
            double rp = parseNum(tbl,"rows_produced_per_join");
            double f  = parseNum(tbl,"filtered");
            auto ci = tbl.contains("cost_info")? tbl["cost_info"]:json({});
            double rc=parseNum(ci,"read_cost");
            double ec=parseNum(ci,"eval_cost");
            double pc=parseNum(ci,"prefix_cost");
            double dr=bytesFrom(ci.value("data_read_per_join","0"));

            /* update aggregates */
            A.reSum+=re; A.rpSum+=rp; A.fSum+=f;
            A.rcSum+=rc; A.ecSum+=ec; A.pcSum+=pc; A.drSum+=dr;
            if(re>0){ double sel=rp/re;
                A.selSum+=sel; A.selMin=std::min(A.selMin,sel);
                A.selMax=std::max(A.selMax,sel);
                A.fanoutMax = std::max(A.fanoutMax, sel);
            }
            double ratio = (ec>0)? rc/ec : rc;
            A.ratioSum += ratio;
            A.ratioMax = std::max(A.ratioMax, ratio);
            A.cnt++;

            /* access type */
            std::string at = tbl.value("access_type","all");
            if(at=="range")      A.cntRange++;
            else if(at=="ref")   A.cntRef++;
            else if(at=="eq_ref")A.cntEq++;
            else if(at=="index") A.cntIdx++;
            else                 A.cntFull++;
            if(tbl.value("using_index",false)) A.usingIdx++;
        }

        /* plan-shape flags */
        if(node.contains("grouping_operation"))      A.hasGroup=true;
        if(node.contains("ordering_operation"))      A.hasOrder=true;
        if(node.value("using_filesort",false))       A.hasOrder=true;
        if(node.value("using_temporary_table",false))A.hasTemp = true;

        /* recurse on children */
        for(const auto& kv: node.items())
            if(kv.key()!="table")
                recursePlan(kv.value(),A,depth+1,
                            parseNum(node.value("table",json({})),"rows_produced_per_join"));
    }
    else if(node.is_array()){
        for(const auto& el: node)
            recursePlan(el,A,depth,parentRP);
    }

    A.maxDepth = std::max(A.maxDepth, depth);
}

/* -------------------------------------------------------------------------- */
bool parseRowPlanJSON(const std::string& path,
                      float outFeats[NUM_FEATS],
                      double& queryCost)
{
    std::fill(outFeats, outFeats+NUM_FEATS, 0.f);
    std::ifstream ifs(path); if(!ifs){ logWarn("open "+path); return false; }
    json j; try{ ifs>>j; }catch(...){ logWarn("json "+path); return false; }
    if(!j.contains("query_block")){ logWarn("no query_block "+path); return false;}

    FeatureAgg A;
    recursePlan(j["query_block"],A,1,0.0);

    queryCost = parseNum(j["query_block"]["cost_info"],"query_cost");

    if(A.cnt==0) return true;             /* rare, keep zeros */

    /* ---------- assemble feature vector ---------- */
    int k=0;
    double invCnt = 1.0 / A.cnt;

    outFeats[k++] = safeLog1p(A.reSum*invCnt);
    outFeats[k++] = safeLog1p(A.rpSum*invCnt);
    outFeats[k++] = safeLog1p(A.fSum *invCnt);
    outFeats[k++] = safeLog1p(A.rcSum*invCnt);
    outFeats[k++] = safeLog1p(A.ecSum*invCnt);
    outFeats[k++] = safeLog1p(A.pcSum*invCnt);
    outFeats[k++] = safeLog1p(A.drSum*invCnt);

    /* access-type ratios */
    outFeats[k++] = float(A.cntRange*invCnt);
    outFeats[k++] = float(A.cntRef  *invCnt);
    outFeats[k++] = float(A.cntEq   *invCnt);
    outFeats[k++] = float(A.cntIdx  *invCnt);
    outFeats[k++] = float(A.cntFull *invCnt);

    /* using-index fraction */
    outFeats[k++] = float(A.usingIdx*invCnt);

    /* selectivity stats */
    outFeats[k++] = float(A.selSum*invCnt);                 /* mean  */
    outFeats[k++] = float(A.selMin);
    outFeats[k++] = float(A.selMax);

    /* join-depth & fanout */
    outFeats[k++] = float(A.maxDepth);
    outFeats[k++] = float(A.fanoutMax);

    /* plan-shape flags */
    outFeats[k++] = A.hasGroup ? 1.f:0.f;
    outFeats[k++] = A.hasOrder ? 1.f:0.f;
    outFeats[k++] = A.hasTemp  ? 1.f:0.f;

    /* read/eval cost ratio */
    outFeats[k++] = float(A.ratioSum*invCnt);               /* mean */
    outFeats[k++] = float(A.ratioMax);

    /* total query cost (log) */
    outFeats[k++] = safeLog1p(queryCost);

    return true;
}

/* -------------------------------------------------------------------------- */
/* ---------------------- BELOW THIS LINE: UNCHANGED ------------------------ */
/* (only occurrences of hard-coded \"8\" replaced with NUM_FEATS)            */
/* -------------------------------------------------------------------------- */

struct RowSampleStruct{
    float feats[NUM_FEATS];
    int   label;       /* 0/1 , -1=ignore */
    double row_time,column_time,original_query_cost;
    int hybrid_use_imci,fann_use_imci;
};


// -----------------------------------------------------------------------------
/*
 * ExecutionPlanDataset Class
 */
// -----------------------------------------------------------------------------
class ExecutionPlanDatasetClass {
private:
    std::vector<RowSampleStruct> data_;
public:
    ExecutionPlanDatasetClass(const std::vector<std::string> &dataDirList, double timeout){
        if(dataDirList.empty()){
            logError("No dataDirList provided.");
            return;
        }

        int skipCount = 0, validCount = 0;

        for(const auto &dDir_name : dataDirList){
            std::string dDir = "/home/wuy/query_costs/" + dDir_name;
            logInfo("=== Loading dataset from directory: " + dDir + " ===");
            std::string csvPath    = dDir + "/query_costs.csv";
            std::string rowPlanDir = dDir + "/row_plans";

            if(!fileExists(csvPath)){
                logWarn("CSV file does not exist: " + csvPath);
                continue;
            }

            DIR *dir = opendir(rowPlanDir.c_str());
            if(dir == nullptr){
                logWarn("Cannot open row_plans directory: " + rowPlanDir);
                continue;
            }
            closedir(dir); // We'll handle individual file checks later

            // count lines
            std::ifstream counter(csvPath.c_str());
            if(!counter.is_open()){
                logError("Cannot open CSV => "+ csvPath);
                return;
            }
            size_t totalLines=0;
            {
                std::string tmp;
                while(std::getline(counter, tmp)) totalLines++;
            }

            // load data
            std::ifstream ifs(csvPath.c_str());
            if(!ifs.is_open()){
                logWarn("Cannot open CSV => " + csvPath);
                continue;
            }
            // skip header
            std::string line;
            std::getline(ifs, line);
            size_t currentLine=0;
            size_t dataLines= (totalLines>0? totalLines-1:0);
            // Process each line
            while(std::getline(ifs, line)){
                currentLine++;
                showProgressBar(currentLine, dataLines);
                if(line.empty()) continue;
                std::stringstream ss(line);
                std::string qidStr, labelStr, rowTimeStr, columnTimeStr, hybridUseImciStr, fannUseImciStr;
                std::getline(ss, qidStr, ',');
                std::getline(ss, labelStr, ',');
                std::getline(ss, rowTimeStr, ',');
                std::getline(ss, columnTimeStr, ',');
                std::getline(ss, hybridUseImciStr, ',');
                std::getline(ss, fannUseImciStr, ',');

                if(qidStr.empty() || labelStr.empty()){
                    skipCount++;
                    continue;
                }

                int lab = 0;
                double rowTime = 0.0, columnTime = 0.0;
                lab = std::stoi(labelStr);
                try{
                    rowTime = std::stod(rowTimeStr);
                }
                catch(...){
                    continue;
                    rowTime = timeout;
                }
                try{
                    columnTime = std::stod(columnTimeStr);
                }
                catch(...){
                    continue;
                    columnTime = timeout;
                }
                if(std::fabs(rowTime - columnTime) < 1e-3){
                    lab = -1; // Ignore queries with no difference
                }
              
                int hybridUseImci = std::stoi(hybridUseImciStr);

                int fannUseImci   = std::stoi(fannUseImciStr);

                std::string planPath = rowPlanDir + "/" + qidStr + ".json";
                if(!fileExists(planPath)){
                    skipCount++;
                    continue;
                }

                float feats[9];
                double original_query_cost = 0.0;
                if(!parseRowPlanJSON(planPath, feats, original_query_cost)){
                    skipCount++;
                    continue;
                }

                RowSampleStruct rs;
                for(int i=0; i<NUM_FEATS; i++) rs.feats[i] = feats[i];
                rs.label = lab;
                rs.row_time = rowTime;
                rs.column_time = columnTime;
                rs.original_query_cost = original_query_cost;
                rs.hybrid_use_imci = hybridUseImci;
                rs.fann_use_imci   = fannUseImci;
                data_.push_back(rs);
                validCount++;
            }
        }

        logInfo("Total valid samples => " + std::to_string(validCount) +
                ", skipped => " + std::to_string(skipCount));
    }

    size_t size() const { return data_.size(); }
    const RowSampleStruct& operator[](size_t idx) const { return data_[idx]; }
    const std::vector<RowSampleStruct>& getData() const { return data_; }
};

// -----------------------------------------------------------------------------
/*
 * WeightedRandomSampler Class
 */
// -----------------------------------------------------------------------------
class WeightedRandomSamplerClass {
private:
    std::vector<double> weights_;
    std::mt19937 rng_;
    std::discrete_distribution<> dist_;
public:
    WeightedRandomSamplerClass(const std::vector<int> &labels){
        int count0 = 0, count1 = 0;
        for(auto lb : labels){
            if(lb == 0) count0++;
            else if(lb == 1) count1++;
        }

        double w0 = (count0 > 0) ? 1.0 / count0 : 1.0;
        double w1 = (count1 > 0) ? 1.0 / count1 : 1.0;

        for(auto lb : labels){
            if(lb == 0){
                weights_.push_back(w0);
            }
            else if(lb == 1){
                weights_.push_back(w1);
            }
            // Ignore labels == -1
        }

        dist_ = std::discrete_distribution<>(weights_.begin(), weights_.end());
        rng_.seed(std::random_device{}());
    }

    size_t sample(){
        return dist_(rng_);
    }
};

// -----------------------------------------------------------------------------
/*
 * Define a Row Sample for Training
 */
// -----------------------------------------------------------------------------
struct TrainSampleStruct{
    std::vector<float> input; // 9 features
    float desired;            // 0.0 or 1.0
};

// -----------------------------------------------------------------------------
/*
 * Prepare Training Samples
 */
// -----------------------------------------------------------------------------
std::vector<TrainSampleStruct> prepareTrainingSamples(const ExecutionPlanDatasetClass &ds, 
                                               const std::vector<size_t> &trainIdx){
    std::vector<TrainSampleStruct> samples;
    for(auto idx : trainIdx){
        const RowSampleStruct &rs = ds[idx];
        if(rs.label == -1) continue; // Ignore
        TrainSampleStruct ts;
        ts.input.assign(rs.feats, rs.feats + NUM_FEATS);
        ts.desired = (rs.label == 1) ? 1.0f : 0.0f;
        samples.push_back(ts);
    }
    return samples;
}

// -----------------------------------------------------------------------------
/*
 * Evaluation Metrics
 */
// -----------------------------------------------------------------------------
struct Metrics{
    double loss;
    double accuracy;
    double precision;
    double recall;
    double f1;
    double auc;
    double recall_neg;
    double recall_pos;
    double precision_neg;
    double precision_pos;
    std::vector<int> ytrue;
    std::vector<float> scores;
    float best_threshold;
};

// Function to compute AUC using trapezoidal rule
double computeAUC(const std::vector<std::pair<double, double>> &rocPoints){
    double auc = 0.0;
    for(size_t i=1; i < rocPoints.size(); i++){
        double dx = rocPoints[i].first - rocPoints[i-1].first;
        double midy = (rocPoints[i].second + rocPoints[i-1].second) / 2.0;
        // std::cout << "dx = " << dx << ", midy = " << midy << std::endl;
        auc += dx * midy;
    }
    return auc;
}

// Function to compute ROC curve points
std::vector<std::pair<double, double>> computeRocCurve(const std::vector<int> &ytrue, const std::vector<float> &scores){
    std::vector<std::pair<double, double>> rocPoints;
    for(int i=0; i<=100; i++){
        double thr = 1.f - double(i) / 100.0;
        int TP=0, FP=0, TN=0, FN=0;
        for(size_t j=0; j< ytrue.size(); j++){
            int pred = (scores[j] >= thr) ? 1 : 0;
            if(ytrue[j] == 1 && pred == 1) TP++;
            if(ytrue[j] == 1 && pred == 0) FN++;
            if(ytrue[j] == 0 && pred == 1) FP++;
            if(ytrue[j] == 0 && pred == 0) TN++;
        }
        double tpr = (TP + FN > 0) ? double(TP) / (TP + FN) : 0.0;
        double fpr = (FP + TN > 0) ? double(FP) / (FP + TN) : 0.0;
        rocPoints.emplace_back(fpr, tpr);
    }
    return rocPoints;
}

// Function to save ROC curve to CSV
void saveRocCurve(const std::vector<std::pair<double, double>> &rocPoints, const std::string &filename){
    std::ofstream ofs(filename.c_str());
    ofs << "FPR,TPR\n";
    for(const auto &p : rocPoints){
        ofs << p.first << "," << p.second << "\n";
    }
    ofs.close();
    logInfo("Saved ROC curve to " + filename);
}

// Function to find best threshold by F1-score
double findBestThreshold(const std::vector<int> &ytrue, const std::vector<float> &scores, double &bestF1){
    double bestThr = 0.5;
    bestF1 = -1.0;

    for(int i=0; i<=100; i++){
        double thr = double(i) / 100.0;
        int TP=0, FP=0, TN=0, FN=0;
        for(size_t j=0; j < ytrue.size(); j++){
            int pred = (scores[j] >= thr) ? 1 : 0;
            if(ytrue[j] == 1 && pred == 1) TP++;
            if(ytrue[j] == 1 && pred == 0) FN++;
            if(ytrue[j] == 0 && pred == 1) FP++;
            if(ytrue[j] == 0 && pred == 0) TN++;
        }
        double precision = (TP + FP > 0) ? double(TP) / (TP + FP) : 0.0;
        double recall    = (TP + FN > 0) ? double(TP) / (TP + FN) : 0.0;
        double f1 = (precision + recall > 0.0) ? 2.0 * precision * recall / (precision + recall) : 0.0;
        if(f1 > bestF1){
            bestF1 = f1;
            bestThr = thr;
        }
    }
    return bestThr;
}

// -----------------------------------------------------------------------------
/*
 * Define the MLP Model using FANN
 */
// -----------------------------------------------------------------------------
struct FannModel{
    struct fann *ann;
    FannModel(int num_input, int num_hidden, int num_output, float learning_rate){
        ann = fann_create_standard(3, num_input, num_hidden, num_output);
        fann_set_activation_function_hidden(ann, FANN_SIGMOID);
        fann_set_activation_function_output(ann, FANN_SIGMOID);
        fann_set_training_algorithm(ann, FANN_TRAIN_INCREMENTAL);
        fann_set_learning_rate(ann, learning_rate);
    }

    ~FannModel(){
        if(ann) fann_destroy(ann);
    }

    void train(const std::vector<TrainSampleStruct> &samples){
        for(const auto &s : samples){
            // 方法 1: 使用 const_cast
            fann_train(ann, const_cast<fann_type*>(s.input.data()), const_cast<fann_type*>(&s.desired));

            // 或者，方法 2: 使用临时变量
            /*
            std::vector<fann_type> input = s.input; // 复制输入
            fann_type desired = s.desired;          // 复制期望输出
            fann_train(ann, input.data(), &desired);
            */
        }
    }

    float run(const std::vector<float> &input){
        fann_type *output = fann_run(ann, const_cast<fann_type*>(input.data()));
        return static_cast<float>(output[0]);
    }

    double getMSE(){
        return fann_get_MSE(ann);
    }

    void save(const std::string &path){
        fann_save(ann, path.c_str());
    }

    void load(const std::string &path){
        fann_destroy(ann);
        ann = fann_create_from_file(path.c_str());
    }
};

// -----------------------------------------------------------------------------
/*
 * Define the Dataset Splitting Function
 */
// -----------------------------------------------------------------------------
std::pair<std::vector<size_t>, std::vector<size_t>> splitDataset(size_t total, double train_ratio=0.8){
    std::vector<size_t> indices(total);
    for(size_t i=0; i<total; i++) indices[i] = i;

    std::random_device rd;
    std::mt19937 g(rd());
    std::shuffle(indices.begin(), indices.end(), g);

    size_t train_size = static_cast<size_t>(train_ratio * total);
    std::vector<size_t> trainIdx(indices.begin(), indices.begin() + train_size);
    std::vector<size_t> valIdx(indices.begin() + train_size, indices.end());

    return {trainIdx, valIdx};
}

// -----------------------------------------------------------------------------
/*
 * Main Function
 */
// -----------------------------------------------------------------------------
int main(int argc, char *argv[]){
    // Default parameters
    // std::vector<std::string> dataDirs = {"tpch_sf1_templates", "tpch_sf1_templates_index", 
                                        // "tpch_sf1_zsce_index_TP", "tpch_sf100_zsce"};
    std::vector<std::string> dataDirs = {};
    
    int epochs = 10000;
    int hiddenNeurons = 128;
    float learning_rate = 0.001f;
    std::string bestModelPath = "checkpoints/best_mlp_enhanced.net";
    bool skipTrain = false;

    // Parse command-line arguments
    for(int i=1; i<argc; i++){
        if(std::strncmp(argv[i], "--data_dirs=", 12) == 0){
            std::string val = std::string(argv[i] + 12);
            dataDirs.push_back(val);
        }
        else if(std::strncmp(argv[i], "--epochs=", 9) == 0){
            epochs = std::stoi(std::string(argv[i] + 8));
        }
        else if(std::strncmp(argv[i], "--hidden_neurons=", 17) == 0){
            hiddenNeurons = std::stoi(std::string(argv[i] + 17));
        }
        else if(std::strncmp(argv[i], "--lr=", 5) == 0){
            learning_rate = std::stof(std::string(argv[i] + 5));
        }
        else if(std::strncmp(argv[i], "--best_model_path=", 18) == 0){
            bestModelPath = std::string(argv[i] + 18);
        }
        else if(std::strncmp(argv[i], "--skip_train", 12) == 0){
            skipTrain = true;
            logInfo("** Received --skip_train => Skipping training and only evaluating. **");
        }
    }

    logInfo("Best model path => " + bestModelPath);

    if(dataDirs.empty()){
        logWarn("No data_dirs provided. Using default directories.");
        dataDirs = {"tpch_sf1_templates", "tpch_sf1_templates_index", 
                   "tpch_sf1_zsce_index_TP", "tpch_sf100_zsce"};
    }

    double timeout = 60.0; // As defined in Python script
    
    // 1) Build dataset
    ExecutionPlanDatasetClass dataset(dataDirs, timeout);
    size_t dsSize = dataset.size();
    logInfo("Combined dataset size => " + std::to_string(dsSize));
    if(dsSize < 30){
        logError("Not enough data. Exiting.");
        return 1;
    }

    // 2) Split dataset into train and validation
    auto splits = splitDataset(dsSize, 0.8);
    std::vector<size_t> trainIdx = splits.first;
    std::vector<size_t> valIdx   = splits.second;

    logInfo("Train samples = " + std::to_string(trainIdx.size()) + 
            ", Val samples = " + std::to_string(valIdx.size()));

    // 3) Prepare Training Samples
    std::vector<int> trainLabels;
    for(auto idx : trainIdx){
        int lab = dataset[idx].label;
        if(lab != -1){
            trainLabels.push_back(lab);
        }
    }

    WeightedRandomSamplerClass sampler(trainLabels);
    std::vector<TrainSampleStruct> trainingSamples = prepareTrainingSamples(dataset, trainIdx);

    // 4) Initialize or Load Model
    FannModel *model = nullptr;
    if(!skipTrain){
        logInfo("Initializing and training the MLP model.");
        model = new FannModel(NUM_FEATS, hiddenNeurons, 1, learning_rate);
    }
    else{
        logInfo("Skipping training. Loading existing model from " + bestModelPath);
        if(!fileExists(bestModelPath)){
            logError("Model file does not exist: " + bestModelPath);
            return 1;
        }
        model = new FannModel(NUM_FEATS, hiddenNeurons, 1, learning_rate); // Temporary initialization
        model->load(bestModelPath);
    }

    // 5) Training Loop with Early Stopping
    double bestValMSE = 1e9;
    int patience = 20;
    int patienceCount = 0;

    if(!skipTrain){
        for(int epoch=1; epoch <= epochs; epoch++){
            // Training
            size_t trainSampleSize = trainingSamples.size();
            double epochMSE = 0.0;
            for(auto &ts : trainingSamples){
                model->train({ts});
                epochMSE += model->getMSE();
            }
            epochMSE /= trainSampleSize;

            // Validation
            double valMSE = 0.0;
            size_t valSampleSize = 0;
            std::vector<int> valYtrue;
            std::vector<float> valScores;
            for(auto idx : valIdx){
                const RowSampleStruct &rs = dataset[idx];
                if(rs.label == -1) continue; // Ignore
                std::vector<float> input(rs.feats, rs.feats + NUM_FEATS);
                float output = model->run(input);
                valScores.push_back(output);
                valYtrue.push_back(rs.label);
                double desired = (rs.label == 1) ? 1.0 : 0.0;
                double err = output - desired;
                valMSE += (err * err);
                valSampleSize++;
            }
            if(valSampleSize > 0){
                valMSE /= valSampleSize;
            }

            logInfo("Epoch " + std::to_string(epoch) + "/" + std::to_string(epochs) + 
                    " => Train MSE=" + std::to_string(epochMSE) + 
                    ", Val MSE=" + std::to_string(valMSE));

            // Check for improvement
            if(valMSE < bestValMSE){
                bestValMSE = valMSE;
                patienceCount = 0;
                // Create directory if it doesn't exist
                std::string dirPath = bestModelPath.substr(0, bestModelPath.find_last_of("/\\"));
                // Use POSIX mkdir with -p like behavior
                std::string mkdirCmd = "mkdir -p " + dirPath;
                system(mkdirCmd.c_str());
                model->save(bestModelPath);
                logInfo("Saved best model with Val MSE = " + std::to_string(bestValMSE));
            }
            else{
                patienceCount++;
                logInfo("No improvement in Val MSE for " + std::to_string(patienceCount) + " epochs.");
                if(patienceCount >= patience){
                    logInfo("Early stopping triggered.");
                    break;
                }
            }
        }

        // Reload the best model
        delete model;
        model = new FannModel(NUM_FEATS, hiddenNeurons, 1, learning_rate);
        model->load(bestModelPath);
        logInfo("Reloaded the best model from " + bestModelPath);
    }

    // 6) Evaluation on Validation Set
    logInfo("Evaluating the model on the entire dataset.");
    std::vector<int> valYtrue;
    std::vector<float> valScores;
    // allIdx = valIdx + trainIdx;
    valIdx.insert(valIdx.end(), trainIdx.begin(), trainIdx.end());
    for(auto idx : valIdx){
        const RowSampleStruct &rs = dataset[idx];
        if(rs.label == -1) continue; // Ignore
        std::vector<float> input(rs.feats, rs.feats + NUM_FEATS);
        float output = model->run(input);
        valScores.push_back(output);
        valYtrue.push_back(rs.label);
    }

    // Compute Metrics
    double valMSE = 0.0;
    for(size_t i=0; i < valYtrue.size(); i++){
        double desired = (valYtrue[i] == 1) ? 1.0 : 0.0;
        double err = valScores[i] - desired;
        valMSE += (err * err);
    }
    if(valYtrue.size() > 0){
        valMSE /= valYtrue.size();
    }

    // Find Best Threshold by F1-score
    double bestF1 = -1.0;
    double bestThr = findBestThreshold(valYtrue, valScores, bestF1);
    bestThr = 0.5; // Use default threshold for now, manually set
    logInfo("Threshold search => Best Threshold = " + std::to_string(bestThr) + 
            ", Best F1 = " + std::to_string(bestF1));

    // Compute final metrics at best threshold
    Metrics metrics;
    metrics.loss = valMSE;
    metrics.best_threshold = static_cast<float>(bestThr);
    for(size_t i=0; i < valYtrue.size(); i++){
        int pred = (valScores[i] >= bestThr) ? 1 : 0;
        metrics.ytrue.push_back(valYtrue[i]);
        metrics.scores.push_back(valScores[i]);

        if(valYtrue[i] == pred){
            metrics.accuracy += 1.0;
        }
        if(valYtrue[i] == 1 && pred == 1){
            metrics.precision += 1.0;
            metrics.recall += 1.0;
        }
        if(valYtrue[i] == 0 && pred == 1){
            metrics.precision += 1.0;
        }
        if(valYtrue[i] == 1 && pred == 0){
            metrics.recall += 1.0;
        }
        if(valYtrue[i] == 0 && pred == 0){
            // True Negative
        }
    }

    size_t total = valYtrue.size();
    if(total > 0){
        metrics.accuracy /= total;
    }

    // Calculate Precision, Recall, F1
    int TP=0, FP=0, TN=0, FN=0;
    for(size_t i=0; i < valYtrue.size(); i++){
        int t = valYtrue[i];
        int p = (valScores[i] >= bestThr) ? 1 : 0;
        if(t == 1 && p == 1) TP++;
        if(t == 0 && p == 1) FP++;
        if(t == 0 && p == 0) TN++;
        if(t == 1 && p == 0) FN++;
    }

    metrics.accuracy = (double)(TP + TN) / (TP + TN + FP + FN);
    metrics.precision = (TP + FP > 0) ? (double)TP / (TP + FP) : 0.0;
    metrics.recall    = (TP + FN > 0) ? (double)TP / (TP + FN) : 0.0;
    metrics.f1        = (metrics.precision + metrics.recall > 0.0) ? 
                         2.0 * metrics.precision * metrics.recall / (metrics.precision + metrics.recall) : 0.0;

    // Precision and Recall for Negatives
    if((TN + FN) > 0){
        metrics.precision_neg = (double)TN / (TN + FN);
    }
    if((TN + FP) > 0){
        metrics.recall_neg = (double)TN / (TN + FP);
    }

    if((TP + FP) > 0){
        metrics.precision_pos = (double)TP / (TP + FP);
    }
    if((TP + FN) > 0){
        metrics.recall_pos = (double)TP / (TP + FN);
    }

    // Compute AUC
    std::vector<std::pair<double, double>> rocPoints = computeRocCurve(metrics.ytrue, metrics.scores);
    metrics.auc = computeAUC(rocPoints);
    saveRocCurve(rocPoints, "roc_curve.csv");

    logInfo("Final Val => Loss=" + std::to_string(metrics.loss) + 
            ", Acc=" + std::to_string(metrics.accuracy) + 
            ", Prec=" + std::to_string(metrics.precision) + 
            ", Rec=" + std::to_string(metrics.recall) + 
            ", F1=" + std::to_string(metrics.f1) + 
            ", AUC=" + std::to_string(metrics.auc) + 
            ", Best Threshold=" + std::to_string(metrics.best_threshold) + 
            ", RecNeg=" + std::to_string(metrics.recall_neg) + 
            ", RecPos=" + std::to_string(metrics.recall_pos) + 
            ", PrecNeg=" + std::to_string(metrics.precision_neg) + 
            ", PrecPos=" + std::to_string(metrics.precision_pos));

    // 7) Optional: Evaluate on the entire dataset for runtime metrics
    // Note: This part may need adjustments based on your specific requirements
    // Here we compute average runtime based on model predictions

    double cost_threshold_run_time = 0.0;
    double hybrid_opt_run_time = 0.0;
    double fann_model_run_time = 0.0;
    double optimal_run_time = 0.0;
    double ai_classifier_run_time = 0.0;
    double row_execution_time = 0.0;
    double column_execution_time = 0.0;
    

    for(size_t i=0; i < dataset.size(); i++){
        const RowSampleStruct &rs = dataset[i];
        row_execution_time = row_execution_time * i / (i + 1) + rs.row_time / (i + 1);
        column_execution_time = column_execution_time * i / (i + 1) + rs.column_time / (i + 1);

        double original_query_cost = rs.original_query_cost; // This should be captured during parsing if needed

        // Cost Threshold Method
        double cost_threshold_delta = (original_query_cost > 50000.0) ? rs.column_time : rs.row_time;
        cost_threshold_run_time = cost_threshold_run_time * i / (i + 1) + cost_threshold_delta / (i + 1);

        double hybrid_opt_delta = (rs.hybrid_use_imci == 1) ? rs.column_time : rs.row_time;
        hybrid_opt_run_time = hybrid_opt_run_time * i / (i + 1) + hybrid_opt_delta / (i + 1);

        double fann_model_delta = (rs.fann_use_imci == 1) ? rs.column_time : rs.row_time;
        fann_model_run_time = fann_model_run_time * i / (i + 1) + fann_model_delta / (i + 1);

        // Optimal Runtime
        double optimal_delta = std::min(rs.row_time, rs.column_time);
        optimal_run_time = optimal_run_time * i / (i + 1) + optimal_delta / (i + 1);

        // AI Classifier Method
        // If using FANN with sigmoid activation, output > 0.5 indicates class 1
        std::vector<float> input(rs.feats, rs.feats + NUM_FEATS);
        float pred_prob = model->run(input);
        int pred_label = (pred_prob >= 0.5f) ? 1 : 0;
        double ai_delta = (pred_label == 1) ? rs.column_time : rs.row_time;
        // If no valid prediction, fallback to row_time
        ai_classifier_run_time = ai_classifier_run_time * i / (i + 1) + ai_delta / (i + 1);
        if (ai_delta > cost_threshold_delta)
            logInfo("cost_threshold_delta = " + std::to_string(cost_threshold_delta) + 
                ", hybrid_opt_delta = " + std::to_string(hybrid_opt_delta) + 
                ", optimal_delta = " + std::to_string(optimal_delta) + 
                ", ai_delta = " + std::to_string(ai_delta));
    }

    logInfo("Runtime Metrics => Cost Threshold run_time: " + std::to_string(cost_threshold_run_time) + 
            "s, Hybrid Opt run_time: " + std::to_string(hybrid_opt_run_time) + 
            "s, Fann model run_time: " + std::to_string(fann_model_run_time) +
            "s, Optimal run_time: " + std::to_string(optimal_run_time) + 
            "s, AI Classifier run_time: " + std::to_string(ai_classifier_run_time) + 
            "s (threshold 0.5) Row Execution Time: " + std::to_string(row_execution_time) + 
            "s  Column Execution Time: " + std::to_string(column_execution_time) + "s");

    // Cleanup
    delete model;

    return 0;
}

