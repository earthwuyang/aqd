you should first read the files in ~/DB/aqd_pg_duckdb, in that directory AQD is implemented on postgres_scanner extension for duckdb, howebver, in this directory called pg_duckdb_postgres, you are supposed to implement AQD paper (~/DB/aqd_pg_duckdb/main.tex) in a docker container running modified postgres with extension pg_duckdb, you should modify postgres kernel to implement cost-threshold routing and machine learning model-baseed routing, you should modify postgres source code, the entire working pipeline can be similar to the ~/DB/aqd_pg_duckdb, please implement this and finally summarize your steps in README.md and report the offline ML model prediction accuracy  and online batch query dispatching makespan and latency in the README.md. for long running queries like data collection, you should use screen to run script and check the progress from time to time, let's go!

So , to be a highly startted github repo, I do not want to implement a Docker impelmentation that smartly route queries to postgres or duckdb, I also   want to locally run postgres and pg_duckdb extension, please modify postgres kernel to expose over 100 features to log, and run 10000 TP queries and 10000 AP queries for each dataset (for these datasets please consult every file in ~/DB/pg_duckdb_duckdb to see how it fetches over 10 datasets), you should also do this in this pg_duckdb_postgres, then write a c++ framework of training lightgbm to train on the collected data to predict the log transformed gap between the query execution time on postgres and the same query's execution time on duckdb (data import references ~/DB/pg_duckdb/duckdb/import_benchmark_datasets, and query generation references the generate_benchmark_queries.py to generate both AP queries and TP queires), and then do SHAP analysis to find fewer features, and embed these fewer features-based lightgbm inside postgres optimizer kernel to route a query to postgres or to duckdb. So there are three method to compare: pg_duckdb default method (heuristic based), a cost threshold method that compares postgres optimizer estimated cost of the query with a fixed threshold(which can be set updated with a set command in postgres as a variable), and lightgbm-based routing method that runs lightgbm prediction and predict based on lightgbm result. further you are expected to implement the online residual thompson sampling bandit learning and mahalanobis resource regulation as described in main.tex the AQD paper. and then offline training and testing of the lightgbm model and online concurrent query end-to-end test to test the integrated system among the three query dispatch methods.  AFter all this, you can also implement Graph Neural Network in c++ to capture the graph structure of postgres plan (this should be done inside postgres optimizer kernel , utilizing the tree structure of postgres optimizer plan) and train the GNN on collected data (feature exposing might be updated, for example, you might need to expose the JSON structure of execution plan to log and outside trainer trains GNN on these JSON plans, after training, you need to implement the GNN inside postgres kernel to dynamically online dispatch queries), you are supposed to implement a variable for postgres to decide which routing mode it is currently using: default, cost-threshold, lightgbm, graph-neural-network, and compare the performance between these methods.

Newest instruction: collect query execution data, train lightgbm, compile postgres and install pg_duckdb extension that supports default routing, cost-threshold routing, lightgbm routing , and GNN routing, and report lightgbm, GNN classification accuracy, and makespan and latency when concurrent executing queires comparing the four dispatch methods